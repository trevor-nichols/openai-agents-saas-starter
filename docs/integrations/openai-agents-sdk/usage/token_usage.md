Here’s what that `usage` block is, how it’s shaped, and what else (if anything) tracks tokens in that stream.

---

## 1. Schema for the `usage` object

From the final `response.completed` event:

```json
"usage": {
  "input_tokens": 6639,
  "input_tokens_details": { "cached_tokens": 6144 },
  "output_tokens": 62,
  "output_tokens_details": { "reasoning_tokens": 21 },
  "total_tokens": 6701
}
```

### TypeScript-style schema

```ts
// Nested details objects (more keys may appear in future)
interface InputTokensDetails {
  /** Tokens that came from prompt caching for this response. */
  cached_tokens?: number;
  // Future fields may be added by OpenAI
  [key: string]: number | undefined;
}

interface OutputTokensDetails {
  /**
   * Tokens spent on hidden reasoning for reasoning models
   * (not part of visible text, but billable).
   */
  reasoning_tokens?: number;
  // Future fields may be added by OpenAI
  [key: string]: number | undefined;
}

/**
 * Token usage for a single Responses API call.
 * This is the object you see under response.usage on response.completed.
 */
export interface OpenAIResponseUsage {
  /** Total input tokens: system + user + tools + prior context used for this call. */
  input_tokens: number;

  /** Breakdown of input-side token categories. */
  input_tokens_details?: InputTokensDetails;

  /** Total output tokens generated by the model (visible + reasoning). */
  output_tokens: number;

  /** Breakdown of output-side token categories. */
  output_tokens_details?: OutputTokensDetails;

  /** Convenience: input_tokens + output_tokens. */
  total_tokens: number;

  // Forward-compat: OpenAI may add more fields later
  [key: string]: unknown;
}
```

### JSON Schema version (if you want validation)

```json
{
  "title": "OpenAIResponseUsage",
  "type": "object",
  "properties": {
    "input_tokens": {
      "type": "integer",
      "minimum": 0,
      "description": "Total tokens counted as input to this response."
    },
    "input_tokens_details": {
      "type": "object",
      "description": "Breakdown of input tokens.",
      "properties": {
        "cached_tokens": {
          "type": "integer",
          "minimum": 0,
          "description": "Tokens served from prompt cache for this response."
        }
      },
      "additionalProperties": {
        "type": "number"
      }
    },
    "output_tokens": {
      "type": "integer",
      "minimum": 0,
      "description": "Total tokens generated by the model."
    },
    "output_tokens_details": {
      "type": "object",
      "description": "Breakdown of output tokens.",
      "properties": {
        "reasoning_tokens": {
          "type": "integer",
          "minimum": 0,
          "description": "Tokens spent on hidden reasoning."
        }
      },
      "additionalProperties": {
        "type": "number"
      }
    },
    "total_tokens": {
      "type": "integer",
      "minimum": 0,
      "description": "input_tokens + output_tokens."
    }
  },
  "required": ["input_tokens", "output_tokens", "total_tokens"],
  "additionalProperties": true
}
```

---

## 2. Where this appears in your stream

In your NDJSON, usage only shows up on the final `response.completed` event:

* `kind`: `"raw_response_event"`
* `raw_type`: `"response.completed"`
* Path to usage:

  ```text
  payload.response.usage
  # or depending on your decoder:
  event.payload.response.usage
  ```

Earlier events:

* `response.created`
* `response.in_progress`
* `response.output_text.delta`
* `response.code_interpreter_call.*`

…all have `"usage": null` (or omit it). The API only gives you full token counts once the response is finished. ([OpenAI Platform][1])

---

## 3. What each field actually tracks

### `input_tokens`

* Count of **all tokens the model consumed** for this call:

  * system / developer messages
  * user messages
  * any injected tool call / tool result messages
  * prior context if you’re threading via `conversation` / `previous_response_id`
* For prompt-cached calls, this still reflects **total logical input**, not “billed” vs “free” – the breakdown is in `input_tokens_details`. ([OpenAI Platform][1])

### `input_tokens_details.cached_tokens`

* Subset of `input_tokens` that were served from the **prompt cache** (via `prompt_cache_key` / `prompt_cache_retention`). ([OpenAI Platform][1])
* Interpretation:

  * `input_tokens` = cached_tokens + “fresh” tokens.
  * `cached_tokens` is the reusable “prefix” that came from a prior call.
* Good for:

  * measuring cache hit-rate
  * cost estimation (cached vs non-cached portions)
  * debugging whether your prompt_cache setup is actually working.

### `output_tokens`

* **All output tokens generated by the model** for this response:

  * visible assistant text
  * any hidden reasoning tokens for reasoning models (those are **also** counted here)
* If you’re using tools, this includes:

  * tokens in tool call arguments (when the model generates them)
  * any intermediate thinking that counts as output according to the model spec.

### `output_tokens_details.reasoning_tokens`

* Subset of `output_tokens` spent on **hidden reasoning** (chain-of-thought style internal tokens) for reasoning-capable models like `gpt-5.1` when `reasoning.effort` is not `none`. ([OpenAI Platform][2])
* These tokens:

  * are **not** returned in the response text
  * do count toward usage/billing
* This lets you:

  * see how “thinky” a call was
  * correlate reasoning effort settings with cost and latency.

### `total_tokens`

* Simply:

  ```text
  total_tokens = input_tokens + output_tokens
  ```

* Convenient if you just want “how big was this call” in a single number.

---

## 4. Is this the *only* thing that tracks tokens?

Short version:

* **On the raw OpenAI response:** yes, `response.usage` is the only place the API exposes token counts for that call.
* **On the Agents SDK / your agent layer:** no, there’s a higher-level `Usage` object that aggregates across calls.

### 4.1. Within this NDJSON stream

In the JSON you pasted:

* **Only** the final `response.completed` event has:

  ```json
  "usage": {
    "input_tokens": ...,
    "input_tokens_details": {...},
    "output_tokens": ...,
    "output_tokens_details": {...},
    "total_tokens": ...
  }
  ```

* Every other event (`response.created`, deltas, tool call events, lifecycle events) **does not** contain any usage numbers – either missing or `usage: null`.

So if you’re building metrics directly from this wire format, you have to:

* Listen for `raw_type === "response.completed"`
* Pull `payload.response.usage`
* Attach that to whatever run/conversation you’re tracking.

### 4.2. What the Agents SDK adds on top

The OpenAI **Agents SDK** wraps these `usage` objects into an aggregated `Usage` model per run. According to the SDK docs, it tracks: ([OpenAI GitHub][3])

* `requests`: number of API calls made for the run
* `input_tokens`: sum of `usage.input_tokens` across all calls
* `output_tokens`: sum of `usage.output_tokens`
* `total_tokens`: sum of `usage.total_tokens`
* `input_tokens_details.cached_tokens`: aggregated across calls
* `output_tokens_details.reasoning_tokens`: aggregated across calls
* `request_usage_entries`: optional list of per-request usage snapshots

That’s all **derived** from the underlying `response.usage` you see in your log, but at a different granularity:

* Responses API: per **response**
* Agents SDK: per **run** (across multiple responses/tool-handoffs)

If you’re only parsing the NDJSON you posted, the **only token source you have is that `usage` object on `response.completed`**. Anything more granular (per tool call, per phase, etc.) you’ll have to compute yourself from your own metadata.

---

## 5. Practical usage in your code

What I’d do in your infra:

1. **Normalize the path** to usage in your stream handler:

   ```ts
   function extractUsageFromEvent(ev: any): OpenAIResponseUsage | null {
     if (
       ev.kind === "raw_response_event" &&
       ev.raw_type === "response.completed" &&
       ev.payload?.response?.usage
     ) {
       return ev.payload.response.usage as OpenAIResponseUsage;
     }
     return null;
   }
   ```

2. **Persist at conversation/run level** in your `agent_run_events` / `agent_conversations` tables:

   * `request_count`
   * `input_tokens`, `output_tokens`, `total_tokens`
   * `cached_tokens`, `reasoning_tokens`

3. **Expose a clean usage summary** in your own API:

   * one object per run (aggregated)
   * optionally, one per underlying Responses API call.

[1]: https://platform.openai.com/docs/api-reference/responses?utm_source=chatgpt.com "Responses | OpenAI API Reference"
[2]: https://platform.openai.com/docs/guides/reasoning?utm_source=chatgpt.com "Reasoning models - OpenAI API"
[3]: https://openai.github.io/openai-agents-python/ref/usage/?utm_source=chatgpt.com "OpenAI Agents SDK - Usage"
